/*
THIS IS A GENERATED/BUNDLED FILE BY ESBUILD
if you want to view the source, please visit the github repository of this plugin
*/

"use strict";
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);

// main.ts
var main_exports = {};
__export(main_exports, {
  default: () => SpeechToTextPlugin
});
module.exports = __toCommonJS(main_exports);
var import_obsidian = require("obsidian");
var translations = {
  en: {
    ribbonStart: "Start transcription",
    ribbonStop: "Stop transcription",
    statusInactive: "STT inactive",
    statusRecording: "[REC] Recording",
    statusRecordingWithSource: "[REC] Recording | Source: {source}",
    tooltipStart: "Start transcription",
    tooltipStop: "Stop transcription\n{statusLine}",
    startNotice: "Starting transcription from: {source}",
    alreadyRunning: "Transcription is already running.",
    noTracks: "No audio track found in the selected source.",
    startError: "Error accessing the audio source: {error}",
    stopNotice: "Transcription stopped.",
    chunkError: "Error transcribing chunk: {error}",
    sendChunkError: "Configure the STT endpoint in the plugin settings.",
    modalTitle: "Select Audio Source",
    modalSourceLabel: "Audio source",
    modalStatusScanning: "Scanning system...",
    modalHelp: "Choose a window/screen or microphone to capture audio.",
    modalStartButton: "Start Transcription",
    modalDesktopModuleError: "Error: Could not load module for screen capture.",
    modalDesktopLog: "Speech-to-Text: Populating audio sources...",
    modalDesktopError: "Error fetching desktop sources.",
    modalDevicesError: "Error fetching media devices.",
    modalNoSources: "No audio sources found. Check permissions.",
    modalLoaded: "Sources loaded",
    sourcePrefixWindow: "Window",
    sourcePrefixScreen: "Screen",
    settingsTitle: "Speech to Text Settings (external API)",
    settingEndpointName: "STT Endpoint (HTTP)",
    settingEndpointDesc: "URL that will receive audio and return text.",
    settingApiKeyName: "API Key (optional)",
    settingApiKeyDesc: "Sent as Authorization: Bearer <key>.",
    settingModelName: "Model (e.g.: whisper-large-v3)",
    settingModelDesc: "Some APIs require the model field in multipart body.",
    settingLanguageName: "Language (optional)",
    settingLanguageDesc: "language field for the backend (e.g.: pt, en).",
    settingFormDataName: "Send as multipart/form-data",
    settingFormDataDesc: "If off, sends binary blob with Content-Type audio/wav.",
    settingFileFieldName: "File field (form-data)",
    settingFileFieldDesc: "Multipart field name that receives the audio.",
    settingTextFieldName: "Text field in JSON response",
    settingTextFieldDesc: 'Field name with the transcription (fallback to "transcript").',
    settingChunkName: "Chunk duration (ms)",
    settingChunkDesc: "Cut length; smaller values send more requests.",
    settingSilenceName: "Silence RMS threshold",
    settingSilenceDesc: "Drops near-silent chunks; lower values let more noise pass. Default: 0.0015.",
    settingBufferName: "Queued audio chunks",
    settingBufferDesc: "Max chunks waiting to send before discarding excess (avoids lag). Default: 12.",
    settingRepeatsName: "Allowed consecutive repeats",
    settingRepeatsDesc: "How many times the same word may repeat before being filtered. Default: 2.",
    settingUiLanguageName: "Plugin language",
    settingUiLanguageDesc: "UI language. System follows Obsidian language when supported.",
    settingUiOptionSystem: "System",
    settingUiOptionEn: "English (en-US)",
    settingUiOptionPt: "Portuguese (pt-BR)",
    settingInactivityName: "Auto-stop timeout (s)",
    settingInactivityDesc: "Stop capture if no audio is detected for longer than this (seconds). Set 0 to disable.",
    inactivityStopNotice: "Capture stopped because no audio was detected."
  },
  pt: {
    ribbonStart: "Iniciar transcri\xE7\xE3o",
    ribbonStop: "Parar transcri\xE7\xE3o",
    statusInactive: "STT inativo",
    statusRecording: "[REC] Gravando",
    statusRecordingWithSource: "[REC] Gravando | Fonte: {source}",
    tooltipStart: "Iniciar transcri\xE7\xE3o",
    tooltipStop: "Parar transcri\xE7\xE3o\n{statusLine}",
    startNotice: "Iniciando transcri\xE7\xE3o da fonte: {source}",
    alreadyRunning: "A transcri\xE7\xE3o j\xE1 est\xE1 em andamento.",
    noTracks: "Nenhuma trilha de \xE1udio encontrada na fonte selecionada.",
    startError: "Erro ao acessar a fonte de \xE1udio: {error}",
    stopNotice: "Transcri\xE7\xE3o interrompida.",
    chunkError: "Erro ao transcrever chunk: {error}",
    sendChunkError: "Configure o endpoint STT nas configura\xE7\xF5es do plugin.",
    modalTitle: "Selecione a Fonte de \xC1udio",
    modalSourceLabel: "Fonte de \xE1udio",
    modalStatusScanning: "Varredura do sistema...",
    modalHelp: "Escolha uma janela/tela ou microfone para capturar o \xE1udio.",
    modalStartButton: "Iniciar Transcri\xE7\xE3o",
    modalDesktopModuleError: "Erro: N\xE3o foi poss\xEDvel carregar o m\xF3dulo para captura de tela.",
    modalDesktopLog: "Speech-to-Text: Populando fontes de \xE1udio...",
    modalDesktopError: "Erro ao buscar fontes do desktop.",
    modalDevicesError: "Erro ao buscar dispositivos de m\xEDdia.",
    modalNoSources: "Nenhuma fonte de \xE1udio encontrada. Verifique permiss\xF5es.",
    modalLoaded: "Fontes carregadas",
    sourcePrefixWindow: "Janela",
    sourcePrefixScreen: "Tela",
    settingsTitle: "Configura\xE7\xF5es do Speech to Text (API externa)",
    settingEndpointName: "Endpoint STT (HTTP)",
    settingEndpointDesc: "URL que receber\xE1 o \xE1udio e retornar\xE1 o texto.",
    settingApiKeyName: "API Key (opcional)",
    settingApiKeyDesc: "Enviada como Authorization: Bearer <chave>.",
    settingModelName: "Modelo (ex.: whisper-large-v3)",
    settingModelDesc: "Algumas APIs exigem o campo model no corpo multipart.",
    settingLanguageName: "Idioma (opcional)",
    settingLanguageDesc: "Campo language para o backend (ex.: pt, en).",
    settingFormDataName: "Enviar como multipart/form-data",
    settingFormDataDesc: "Se desativado, envia o blob bin\xE1rio com Content-Type audio/wav.",
    settingFileFieldName: "Campo do arquivo (form-data)",
    settingFileFieldDesc: "Nome do campo multipart que receber\xE1 o \xE1udio.",
    settingTextFieldName: "Campo do texto na resposta JSON",
    settingTextFieldDesc: 'Nome do campo com a transcri\xE7\xE3o (fallback para "transcript").',
    settingChunkName: "Dura\xE7\xE3o do chunk (ms)",
    settingChunkDesc: "Tempo de corte; valores menores enviam mais requisi\xE7\xF5es.",
    settingSilenceName: "Limite RMS de sil\xEAncio",
    settingSilenceDesc: "Descarta chunks quase silenciosos; valores menores deixam passar mais ru\xEDdo. Padr\xE3o: 0.0015.",
    settingBufferName: "Buffers de \xE1udio em fila",
    settingBufferDesc: "Limite de chunks aguardando envio antes de descartar o excesso (evita travar). Padr\xE3o: 12.",
    settingRepeatsName: "Repeti\xE7\xF5es consecutivas permitidas",
    settingRepeatsDesc: "Quantas vezes a mesma palavra pode se repetir em sequ\xEAncia antes de ser filtrada. Padr\xE3o: 2.",
    settingUiLanguageName: "Idioma do plugin",
    settingUiLanguageDesc: "Idioma da interface. Sistema segue o idioma do Obsidian quando suportado.",
    settingUiOptionSystem: "Sistema",
    settingUiOptionEn: "Ingl\xEAs (en-US)",
    settingUiOptionPt: "Portugu\xEAs (pt-BR)",
    settingInactivityName: "Tempo para desligar (s)",
    settingInactivityDesc: "Desliga a captura se nenhum \xE1udio for detectado por mais tempo que isso (segundos). Use 0 para desativar.",
    inactivityStopNotice: "Captura desligada por inatividade (sem \xE1udio detectado)."
  }
};
var SpeechToTextPlugin = class extends import_obsidian.Plugin {
  constructor() {
    super(...arguments);
    this.isTranscribing = false;
    this.currentStream = null;
    this.audioContext = null;
    this.audioProcessor = null;
    this.audioSource = null;
    this.ribbonIconEl = null;
    this.statusBarItem = null;
    this.activeSourceName = null;
    this.sampleBuffers = [];
    this.samplesCollected = 0;
    this.targetSampleRate = 16e3;
    this.maxBufferedChunks = 12;
    this.maxConsecutiveWordRepeats = 2;
    this.currentLang = "en";
    this.lastTranscriptTail = "";
    this.lastAudioDetectedAt = null;
    // Small noise gate to avoid sending pure silence to the STT backend
    this.silenceRmsThreshold = 15e-4;
    this.pendingControllers = [];
    this.inactivityTimeoutId = null;
  }
  async onload() {
    var _a;
    this.settings = await this.loadSettings();
    this.applyTuningSettings();
    this.addSettingTab(new STTSettingTab(this.app, this));
    this.injectStyles();
    this.statusBarItem = this.addStatusBarItem();
    this.updateStatusBar(this.t("statusInactive"));
    this.ribbonIconEl = this.addRibbonIcon("mic", this.t("ribbonStart"), () => {
      if (this.isTranscribing) {
        this.stopTranscription();
        return;
      }
      new AudioSourceSelectorModal(this.app, this, (source) => {
        this.startTranscription(source);
      }).open();
    });
    (_a = this.ribbonIconEl) == null ? void 0 : _a.addClass("stt-ribbon-icon");
    this.setRibbonTooltip(this.t("tooltipStart"));
    this.addCommand({
      id: "start-transcription",
      name: this.t("ribbonStart"),
      callback: () => {
        if (this.isTranscribing) {
          new import_obsidian.Notice(this.t("alreadyRunning"));
          return;
        }
        new AudioSourceSelectorModal(this.app, this, (source) => {
          this.startTranscription(source);
        }).open();
      }
    });
    this.applyLanguage();
  }
  async startTranscription(source) {
    if (this.isTranscribing)
      return;
    new import_obsidian.Notice(this.t("startNotice", { source: source.name }));
    this.isTranscribing = true;
    this.lastTranscriptTail = "";
    try {
      let stream;
      if (source.id.startsWith("window:") || source.id.startsWith("screen:")) {
        stream = await navigator.mediaDevices.getUserMedia({
          audio: {
            mandatory: {
              chromeMediaSource: "desktop",
              chromeMediaSourceId: source.id
            }
          },
          video: {
            mandatory: {
              chromeMediaSource: "desktop",
              chromeMediaSourceId: source.id
            }
          }
        });
      } else {
        stream = await navigator.mediaDevices.getUserMedia({
          audio: { deviceId: { exact: source.id } },
          video: false
        });
      }
      const audioTracks = stream.getAudioTracks();
      if (!audioTracks || audioTracks.length === 0) {
        throw new Error(this.t("noTracks"));
      }
      this.currentStream = stream;
      this.activeSourceName = source.name;
      this.markRecording(true);
      await this.startPCMRecorder(stream);
    } catch (error) {
      console.error("Speech-to-Text: Failed to start transcription:", error);
      new import_obsidian.Notice(this.t("startError", { error: (error == null ? void 0 : error.message) || String(error) }));
      this.stopTranscription();
    }
  }
  stopTranscription(showNotice = true) {
    if (!this.isTranscribing)
      return;
    if (this.audioProcessor) {
      this.audioProcessor.disconnect();
      this.audioProcessor.onaudioprocess = null;
      this.audioProcessor = null;
    }
    if (this.audioSource) {
      this.audioSource.disconnect();
      this.audioSource = null;
    }
    if (this.audioContext) {
      this.audioContext.close().catch(() => null);
      this.audioContext = null;
    }
    if (this.currentStream) {
      this.currentStream.getTracks().forEach((track) => track.stop());
      this.currentStream = null;
    }
    this.activeSourceName = null;
    this.markRecording(false);
    this.sampleBuffers = [];
    this.samplesCollected = 0;
    this.lastTranscriptTail = "";
    this.pendingControllers.forEach((controller) => controller.abort());
    this.pendingControllers = [];
    this.clearInactivityTimer();
    this.lastAudioDetectedAt = null;
    this.isTranscribing = false;
    if (showNotice) {
      new import_obsidian.Notice(this.t("stopNotice"));
    }
  }
  async startPCMRecorder(stream) {
    if (this.audioContext) {
      this.stopTranscription();
    }
    const audioContext = new AudioContext({ sampleRate: this.targetSampleRate });
    this.targetSampleRate = audioContext.sampleRate;
    this.audioContext = audioContext;
    const sourceNode = audioContext.createMediaStreamSource(stream);
    this.audioSource = sourceNode;
    const processor = audioContext.createScriptProcessor(4096, 1, 1);
    this.audioProcessor = processor;
    let baseOffset = null;
    const editorGetter = () => {
      var _a;
      return (_a = this.app.workspace.activeEditor) == null ? void 0 : _a.editor;
    };
    const samplesPerChunk = Math.max(1, Math.round(this.targetSampleRate * this.settings.chunkMs / 1e3));
    this.lastAudioDetectedAt = Date.now();
    this.restartInactivityTimer();
    processor.onaudioprocess = async (event) => {
      const channelData = event.inputBuffer.getChannelData(0);
      const clone = new Float32Array(channelData.length);
      clone.set(channelData);
      this.sampleBuffers.push(clone);
      this.samplesCollected += clone.length;
      this.trimBacklog(samplesPerChunk);
      while (this.samplesCollected >= samplesPerChunk) {
        const chunkSamples = this.takeSamples(samplesPerChunk);
        const isSilent = this.isMostlySilence(chunkSamples);
        this.handleInactivity(!isSilent);
        if (isSilent) {
          continue;
        }
        const wavBlob = this.encodeWav(chunkSamples, this.targetSampleRate);
        try {
          const transcript = await this.sendChunk(wavBlob, "audio/wav");
          const editor = editorGetter();
          if (!transcript || !editor)
            continue;
          let insertText = this.cleanTranscript(transcript, this.lastTranscriptTail);
          if (!insertText)
            continue;
          if (baseOffset === null) {
            baseOffset = editor.posToOffset(editor.getCursor());
          }
          if (baseOffset > 0 && insertText && !/^\s/.test(insertText)) {
            const prevChar = editor.getRange(
              editor.offsetToPos(baseOffset - 1),
              editor.offsetToPos(baseOffset)
            );
            if (prevChar && !/\s/.test(prevChar)) {
              insertText = " " + insertText;
            }
          }
          const insertPos = editor.offsetToPos(baseOffset);
          editor.replaceRange(insertText, insertPos);
          baseOffset += insertText.length;
          this.lastTranscriptTail = this.updateTail(this.lastTranscriptTail, insertText);
          editor.setCursor(editor.offsetToPos(baseOffset));
        } catch (sendError) {
          console.error("Speech-to-Text: Erro ao enviar chunk PCM para STT:", sendError);
          new import_obsidian.Notice(this.t("chunkError", { error: (sendError == null ? void 0 : sendError.message) || String(sendError) }));
          this.stopTranscription();
        }
      }
    };
    sourceNode.connect(processor);
    processor.connect(audioContext.destination);
    console.log("Speech-to-Text: Captura PCM iniciada com SampleRate", this.targetSampleRate);
  }
  takeSamples(count) {
    const out = new Float32Array(count);
    let offset = 0;
    while (offset < count && this.sampleBuffers.length > 0) {
      const buffer = this.sampleBuffers[0];
      const needed = count - offset;
      if (buffer.length <= needed) {
        out.set(buffer, offset);
        offset += buffer.length;
        this.sampleBuffers.shift();
      } else {
        out.set(buffer.subarray(0, needed), offset);
        this.sampleBuffers[0] = buffer.subarray(needed);
        offset += needed;
      }
    }
    this.samplesCollected -= count;
    return out;
  }
  trimBacklog(samplesPerChunk) {
    const maxSamples = samplesPerChunk * this.maxBufferedChunks;
    if (this.samplesCollected <= maxSamples)
      return;
    const overflow = this.samplesCollected - maxSamples;
    this.discardSamples(overflow);
    console.warn("Speech-to-Text: Buffer overflow, discarding old samples to keep performance:", overflow);
  }
  discardSamples(count) {
    let remaining = count;
    while (remaining > 0 && this.sampleBuffers.length > 0) {
      const buffer = this.sampleBuffers[0];
      if (buffer.length <= remaining) {
        remaining -= buffer.length;
        this.sampleBuffers.shift();
      } else {
        this.sampleBuffers[0] = buffer.subarray(remaining);
        remaining = 0;
      }
    }
    this.samplesCollected = Math.max(0, this.samplesCollected - count);
  }
  isMostlySilence(samples) {
    if (samples.length === 0)
      return true;
    let sumSquares = 0;
    let peak = 0;
    for (let i = 0; i < samples.length; i++) {
      const value = samples[i];
      const abs = Math.abs(value);
      peak = Math.max(peak, abs);
      sumSquares += value * value;
    }
    const rms = Math.sqrt(sumSquares / samples.length);
    return rms < this.silenceRmsThreshold && peak < this.silenceRmsThreshold * 4;
  }
  cleanTranscript(incoming, previousTail) {
    if (!incoming)
      return incoming;
    const normalizedIncoming = incoming.trimStart();
    if (!previousTail) {
      return this.dedupeConsecutiveWords(normalizedIncoming);
    }
    const prevWords = previousTail.trim().split(/\s+/);
    const nextWords = normalizedIncoming.split(/\s+/);
    const maxOverlap = Math.min(6, prevWords.length, nextWords.length);
    for (let overlap = maxOverlap; overlap > 0; overlap--) {
      const tailPhrase = prevWords.slice(-overlap).join(" ").toLowerCase();
      const headPhrase = nextWords.slice(0, overlap).join(" ").toLowerCase();
      if (tailPhrase === headPhrase) {
        const remainder = nextWords.slice(overlap).join(" ");
        return this.dedupeConsecutiveWords(remainder);
      }
    }
    return this.dedupeConsecutiveWords(normalizedIncoming);
  }
  updateTail(previousTail, appended) {
    const combined = previousTail + appended;
    return combined.slice(-200);
  }
  applyTuningSettings() {
    this.silenceRmsThreshold = Number.isFinite(this.settings.silenceThreshold) ? Math.max(0, this.settings.silenceThreshold) : 15e-4;
    this.maxBufferedChunks = Number.isFinite(this.settings.maxBufferedChunks) && this.settings.maxBufferedChunks > 0 ? Math.round(this.settings.maxBufferedChunks) : 12;
    this.maxConsecutiveWordRepeats = Number.isFinite(this.settings.maxConsecutiveWordRepeats) && this.settings.maxConsecutiveWordRepeats >= 1 ? Math.round(this.settings.maxConsecutiveWordRepeats) : 2;
    this.settings.inactivityTimeoutMs = Number.isFinite(this.settings.inactivityTimeoutMs) && this.settings.inactivityTimeoutMs >= 0 ? Math.round(this.settings.inactivityTimeoutMs) : 0;
  }
  resolveLanguage() {
    var _a, _b, _c, _d, _e, _f, _g, _h;
    if (this.settings.uiLanguage === "system") {
      const obsLang = (_h = (_e = (_b = (_a = this.app.vault).getConfig) == null ? void 0 : _b.call(_a, "language")) != null ? _e : (_d = (_c = this.app.vault).getConfig) == null ? void 0 : _d.call(_c, "lang")) != null ? _h : (_g = (_f = this.app.app) == null ? void 0 : _f.loadLocalStorage) == null ? void 0 : _g.call(_f, "language");
      if (typeof obsLang === "string" && obsLang.toLowerCase().startsWith("pt")) {
        return "pt";
      }
      return "en";
    }
    return this.settings.uiLanguage === "pt" ? "pt" : "en";
  }
  applyLanguage() {
    this.currentLang = this.resolveLanguage();
    if (this.isTranscribing) {
      this.markRecording(true);
    } else {
      this.setRibbonTooltip(this.t("tooltipStart"));
      this.updateStatusBar(this.t("statusInactive"));
    }
  }
  handleInactivity(hasAudio) {
    if (this.settings.inactivityTimeoutMs <= 0)
      return;
    const now = Date.now();
    if (hasAudio) {
      this.lastAudioDetectedAt = now;
      this.restartInactivityTimer();
      return;
    }
    if (this.lastAudioDetectedAt === null) {
      this.lastAudioDetectedAt = now;
      this.restartInactivityTimer();
    }
  }
  restartInactivityTimer() {
    this.clearInactivityTimer();
    if (this.settings.inactivityTimeoutMs <= 0 || this.lastAudioDetectedAt === null)
      return;
    this.inactivityTimeoutId = window.setTimeout(() => {
      this.stopTranscription(false);
      new import_obsidian.Notice(this.t("inactivityStopNotice"));
    }, this.settings.inactivityTimeoutMs);
  }
  clearInactivityTimer() {
    if (this.inactivityTimeoutId !== null) {
      window.clearTimeout(this.inactivityTimeoutId);
      this.inactivityTimeoutId = null;
    }
  }
  t(key, vars = {}) {
    var _a, _b;
    const dict = translations[this.currentLang] || translations.en;
    const fallback = translations.en;
    const template = (_b = (_a = dict[key]) != null ? _a : fallback[key]) != null ? _b : key;
    return template.replace(
      /\{(\w+)\}/g,
      (match, k) => vars[k] !== void 0 ? String(vars[k]) : match
    );
  }
  dedupeConsecutiveWords(text) {
    if (!text)
      return text;
    const tokens = text.split(/(\s+)/);
    const output = [];
    let lastWord = "";
    let repeatCount = 0;
    const normalize = (token) => token.toLowerCase().replace(/[^a-z0-9\u00c0-\u017f]+/gi, "");
    for (const token of tokens) {
      if (/^\s+$/.test(token)) {
        output.push(token);
        continue;
      }
      const normalized = normalize(token);
      if (normalized && normalized === lastWord) {
        repeatCount += 1;
        if (repeatCount >= this.maxConsecutiveWordRepeats) {
          continue;
        }
      } else {
        lastWord = normalized;
        repeatCount = 0;
      }
      output.push(token);
    }
    return output.join("").replace(/\s+/g, " ").trim();
  }
  encodeWav(samples, sampleRate) {
    const buffer = new ArrayBuffer(44 + samples.length * 2);
    const view = new DataView(buffer);
    const writeString = (offset, str) => {
      for (let i = 0; i < str.length; i++) {
        view.setUint8(offset + i, str.charCodeAt(i));
      }
    };
    const pcm = new Int16Array(samples.length);
    for (let i = 0; i < samples.length; i++) {
      const s = Math.max(-1, Math.min(1, samples[i]));
      pcm[i] = s < 0 ? s * 32768 : s * 32767;
    }
    writeString(0, "RIFF");
    view.setUint32(4, 36 + pcm.length * 2, true);
    writeString(8, "WAVE");
    writeString(12, "fmt ");
    view.setUint32(16, 16, true);
    view.setUint16(20, 1, true);
    view.setUint16(22, 1, true);
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, sampleRate * 2, true);
    view.setUint16(32, 2, true);
    view.setUint16(34, 16, true);
    writeString(36, "data");
    view.setUint32(40, pcm.length * 2, true);
    for (let i = 0; i < pcm.length; i++) {
      view.setInt16(44 + i * 2, pcm[i], true);
    }
    return new Blob([buffer], { type: "audio/wav" });
  }
  async sendChunk(blob, mimeTypeUsed) {
    var _a, _b;
    if (!this.settings.endpointUrl) {
      throw new Error("Configure o endpoint STT nas configura\xE7\xF5es do plugin.");
    }
    const controller = new AbortController();
    this.pendingControllers.push(controller);
    const headers = {};
    if (!this.settings.useFormData) {
      headers["Content-Type"] = mimeTypeUsed;
    }
    if (this.settings.apiKey) {
      headers["Authorization"] = `Bearer ${this.settings.apiKey}`;
    }
    const body = this.settings.useFormData ? (() => {
      const form = new FormData();
      form.append(this.settings.fileField || "file", blob, "audio.wav");
      if (this.settings.model) {
        form.append("model", this.settings.model);
      }
      if (this.settings.language) {
        form.append("language", this.settings.language);
      }
      return form;
    })() : blob;
    const response = await fetch(this.settings.endpointUrl, {
      method: "POST",
      headers,
      body,
      signal: controller.signal
    });
    this.pendingControllers = this.pendingControllers.filter((c) => c !== controller);
    if (!response.ok) {
      const text = await response.text();
      throw new Error(`HTTP ${response.status}: ${text}`);
    }
    const contentType = response.headers.get("Content-Type") || "";
    if (contentType.includes("application/json")) {
      const json = await response.json();
      const field = this.settings.textField || "text";
      const result = (_b = (_a = json[field]) != null ? _a : json.transcript) != null ? _b : "";
      return typeof result === "string" ? result : JSON.stringify(result);
    }
    return await response.text();
  }
  async loadSettings() {
    const defaultSettings = {
      endpointUrl: "",
      apiKey: "",
      useFormData: true,
      model: "",
      language: "",
      fileField: "file",
      textField: "text",
      chunkMs: 4e3,
      silenceThreshold: 15e-4,
      maxBufferedChunks: 12,
      maxConsecutiveWordRepeats: 2,
      uiLanguage: "en",
      inactivityTimeoutMs: 0
    };
    const loaded = await this.loadData();
    return Object.assign({}, defaultSettings, loaded);
  }
  async saveSettings() {
    await this.saveData(this.settings);
  }
  onunload() {
    this.stopTranscription();
    const style = document.getElementById("stt-plugin-styles");
    if (style)
      style.remove();
  }
  markRecording(active) {
    var _a, _b;
    if (active) {
      (_a = this.ribbonIconEl) == null ? void 0 : _a.addClass("stt-recording");
      const status = this.activeSourceName ? this.t("statusRecordingWithSource", { source: this.activeSourceName }) : this.t("statusRecording");
      const tooltip = this.t("tooltipStop", { statusLine: status });
      this.setRibbonTooltip(tooltip);
      this.updateStatusBar(status);
    } else {
      (_b = this.ribbonIconEl) == null ? void 0 : _b.removeClass("stt-recording");
      this.setRibbonTooltip(this.t("tooltipStart"));
      this.updateStatusBar(this.t("statusInactive"));
    }
  }
  setRibbonTooltip(text) {
    if (!this.ribbonIconEl)
      return;
    this.ribbonIconEl.setAttribute("aria-label", text);
    this.ribbonIconEl.setAttribute("data-tooltip", text);
  }
  updateStatusBar(text) {
    if (this.statusBarItem) {
      this.statusBarItem.setText(text);
    }
  }
  injectStyles() {
    const style = document.createElement("style");
    style.id = "stt-plugin-styles";
    style.textContent = `
        .stt-ribbon-icon.stt-recording {
            color: var(--text-accent);
            background: var(--background-modifier-success);
            border-radius: 6px;
        }
        .tooltip {
            text-align: left;
            white-space: pre-wrap;
            word-wrap: break-word;
            max-width: 320px;
        }
        .stt-source-modal .source-row {
            display: flex;
            flex-direction: column;
            gap: 6px;
            margin-bottom: 16px;
        }
        .stt-source-modal {
            padding: 12px 16px 16px 16px;
        }
        .stt-source-modal h2 {
            margin-top: 4px;
            margin-bottom: 10px;
            font-size: 20px;
            font-weight: 700;
            color: var(--text-normal);
        }
        .stt-divider {
            border-top: 1px solid var(--background-modifier-border);
            margin: 6px 0 14px 0;
        }
        .stt-field {
            display: flex;
            flex-direction: column;
            gap: 8px;
        }
        .stt-label-row {
            display: flex;
            justify-content: space-between;
            align-items: baseline;
            gap: 12px;
        }
        .stt-label {
            font-weight: 600;
            font-size: 16px;
            color: var(--text-normal);
        }
        .stt-status {
            color: var(--text-faint);
            font-size: 12px;
        }
        .stt-help {
            color: var(--text-muted);
            font-size: 13px;
        }
        .stt-actions {
            margin-top: 14px;
            display: flex;
            justify-content: flex-start;
        }
        .stt-source-modal .status {
            color: var(--text-faint);
            font-size: 12px;
        }
        .stt-source-modal select {
            width: 100%;
            max-width: 260px;
        }
        `;
    document.head.appendChild(style);
  }
};
var AudioSourceSelectorModal = class extends import_obsidian.Modal {
  constructor(app, plugin, onSubmit) {
    super(app);
    this.isLoading = true;
    this.selectEl = null;
    this.startButton = null;
    this.statusText = null;
    this.sources = [];
    this.onSubmit = onSubmit;
    this.plugin = plugin;
  }
  async onOpen() {
    const { contentEl } = this;
    contentEl.empty();
    contentEl.addClass("stt-source-modal");
    contentEl.createEl("h2", { text: this.plugin.t("modalTitle") });
    contentEl.createDiv({ cls: "stt-divider" });
    const field = contentEl.createDiv({ cls: "stt-field" });
    const labelRow = field.createDiv({ cls: "stt-label-row" });
    labelRow.createEl("div", { text: this.plugin.t("modalSourceLabel"), cls: "stt-label" });
    this.statusText = labelRow.createEl("div", { cls: "stt-status", text: this.plugin.t("modalStatusScanning") });
    field.createEl("div", {
      text: this.plugin.t("modalHelp"),
      cls: "stt-help"
    });
    this.selectEl = field.createEl("select", { cls: "dropdown" });
    const actions = contentEl.createDiv({ cls: "stt-actions" });
    this.startButton = actions.createEl("button", { text: this.plugin.t("modalStartButton"), cls: "mod-cta" });
    this.selectEl.disabled = true;
    this.startButton.disabled = true;
    this.startButton.onclick = () => {
      var _a;
      const selectedSourceId = (_a = this.selectEl) == null ? void 0 : _a.value;
      const selectedSource = this.sources.find((s) => s.id === selectedSourceId);
      if (selectedSource) {
        this.onSubmit(selectedSource);
        this.close();
      }
    };
    await this.populateSources();
  }
  async populateSources() {
    var _a, _b, _c, _d, _e, _f;
    console.log("Speech-to-Text: Populando fontes de \xE1udio...");
    const electron = window.require("electron");
    const remote = electron.remote;
    if (!remote || !remote.desktopCapturer) {
      console.error("Speech-to-Text: M\xF3dulo desktopCapturer do Electron n\xE3o encontrado.");
      (_a = this.statusText) == null ? void 0 : _a.setText("Erro: N\xE3o foi poss\xEDvel carregar o m\xF3dulo para captura de tela.");
      return;
    }
    const { desktopCapturer } = remote;
    const audioSources = [];
    try {
      const sources = await desktopCapturer.getSources({ types: ["window", "screen"] });
      const audioKeywords = [
        "chrome",
        "edge",
        "firefox",
        "brave",
        "opera",
        "spotify",
        "music",
        "player",
        "vlc",
        "teams",
        "zoom",
        "meet",
        "webex",
        "skype",
        "discord",
        "youtube",
        "netflix",
        "prime video",
        "hbo",
        "disney",
        "call",
        "meeting",
        "conference",
        "stream"
      ];
      const looksAudioCapable = (name) => {
        const lowered = name.toLowerCase();
        return audioKeywords.some((kw) => lowered.includes(kw));
      };
      const filteredSources = sources.filter((source) => {
        if (!source.name || source.name.includes("Obsidian"))
          return false;
        if (source.id.startsWith("screen:"))
          return true;
        return looksAudioCapable(source.name);
      });
      filteredSources.forEach((source) => {
        const labelPrefix = source.id.startsWith("screen:") ? this.plugin.t("sourcePrefixScreen") : this.plugin.t("sourcePrefixWindow");
        audioSources.push({ id: source.id, name: `${labelPrefix}: ${source.name}` });
      });
      console.log(
        "Speech-to-Text: Fontes de tela/janela filtradas",
        filteredSources.length,
        "de",
        sources.length
      );
    } catch (error) {
      console.error(this.plugin.t("modalDesktopError"), error);
      (_b = this.statusText) == null ? void 0 : _b.setText(this.plugin.t("modalDesktopError"));
      (_c = this.statusText) == null ? void 0 : _c.setText(this.plugin.t("modalDesktopError"));
      (_d = this.statusText) == null ? void 0 : _d.setText(this.plugin.t("modalDesktopError"));
    }
    try {
      const devices = await navigator.mediaDevices.enumerateDevices();
      const audioInputs = devices.filter((device) => device.kind === "audioinput");
      audioInputs.forEach((device, index) => {
        audioSources.push({ id: device.deviceId, name: device.label || `Microfone ${index + 1}` });
      });
    } catch (error) {
      console.error("Speech-to-Text: Erro ao buscar dispositivos de m\xEDdia:", error);
    }
    this.sources = audioSources;
    if (audioSources.length === 0) {
      (_e = this.statusText) == null ? void 0 : _e.setText("Nenhuma fonte de \xE1udio encontrada. Verifique permiss\xF5es.");
      return;
    }
    if (this.selectEl) {
      this.selectEl.empty();
      audioSources.forEach((source) => {
        var _a2;
        (_a2 = this.selectEl) == null ? void 0 : _a2.createEl("option", { text: source.name, value: source.id });
      });
      this.selectEl.disabled = false;
    }
    if (this.startButton) {
      this.startButton.disabled = false;
    }
    (_f = this.statusText) == null ? void 0 : _f.setText(this.plugin.t("modalLoaded"));
  }
  onClose() {
    const { contentEl } = this;
    contentEl.empty();
  }
};
var STTSettingTab = class extends import_obsidian.PluginSettingTab {
  constructor(app, plugin) {
    super(app, plugin);
    this.plugin = plugin;
  }
  display() {
    const { containerEl } = this;
    containerEl.empty();
    const t = this.plugin.t.bind(this.plugin);
    containerEl.createEl("h2", { text: t("settingsTitle") });
    new import_obsidian.Setting(containerEl).setName(t("settingUiLanguageName")).setDesc(t("settingUiLanguageDesc")).addDropdown(
      (dropdown) => dropdown.addOption("system", t("settingUiOptionSystem")).addOption("en", t("settingUiOptionEn")).addOption("pt", t("settingUiOptionPt")).setValue(this.plugin.settings.uiLanguage).onChange(async (value) => {
        this.plugin.settings.uiLanguage = value === "pt" || value === "system" ? value : "en";
        await this.plugin.saveSettings();
        this.plugin.applyLanguage();
      })
    );
    new import_obsidian.Setting(containerEl).setName(t("settingEndpointName")).setDesc(t("settingEndpointDesc")).addText(
      (text) => text.setPlaceholder("https://my-api.stt/transcribe").setValue(this.plugin.settings.endpointUrl).onChange(async (value) => {
        this.plugin.settings.endpointUrl = value.trim();
        await this.plugin.saveSettings();
      })
    );
    new import_obsidian.Setting(containerEl).setName(t("settingApiKeyName")).setDesc(t("settingApiKeyDesc")).addText(
      (text) => text.setPlaceholder("api-key").setValue(this.plugin.settings.apiKey).onChange(async (value) => {
        this.plugin.settings.apiKey = value.trim();
        await this.plugin.saveSettings();
      })
    );
    new import_obsidian.Setting(containerEl).setName(t("settingModelName")).setDesc(t("settingModelDesc")).addText(
      (text) => text.setPlaceholder("whisper-large-v3").setValue(this.plugin.settings.model).onChange(async (value) => {
        this.plugin.settings.model = value.trim();
        await this.plugin.saveSettings();
      })
    );
    new import_obsidian.Setting(containerEl).setName(t("settingLanguageName")).setDesc(t("settingLanguageDesc")).addText(
      (text) => text.setPlaceholder("pt").setValue(this.plugin.settings.language).onChange(async (value) => {
        this.plugin.settings.language = value.trim();
        await this.plugin.saveSettings();
      })
    );
    new import_obsidian.Setting(containerEl).setName(t("settingFormDataName")).setDesc(t("settingFormDataDesc")).addToggle(
      (toggle) => toggle.setValue(this.plugin.settings.useFormData).onChange(async (value) => {
        this.plugin.settings.useFormData = value;
        await this.plugin.saveSettings();
      })
    );
    new import_obsidian.Setting(containerEl).setName(t("settingFileFieldName")).setDesc(t("settingFileFieldDesc")).addText(
      (text) => text.setPlaceholder("file").setValue(this.plugin.settings.fileField).onChange(async (value) => {
        this.plugin.settings.fileField = value || "file";
        await this.plugin.saveSettings();
      })
    );
    new import_obsidian.Setting(containerEl).setName(t("settingTextFieldName")).setDesc(t("settingTextFieldDesc")).addText(
      (text) => text.setPlaceholder("text").setValue(this.plugin.settings.textField).onChange(async (value) => {
        this.plugin.settings.textField = value || "text";
        await this.plugin.saveSettings();
      })
    );
    new import_obsidian.Setting(containerEl).setName(t("settingChunkName")).setDesc(t("settingChunkDesc")).addText(
      (text) => text.setPlaceholder("4000").setValue(String(this.plugin.settings.chunkMs)).onChange(async (value) => {
        const num = Number(value);
        this.plugin.settings.chunkMs = Number.isFinite(num) && num > 500 ? num : 4e3;
        await this.plugin.saveSettings();
        this.plugin.applyTuningSettings();
      })
    );
    new import_obsidian.Setting(containerEl).setName(t("settingSilenceName")).setDesc(t("settingSilenceDesc")).addText(
      (text) => text.setPlaceholder("0.0015").setValue(String(this.plugin.settings.silenceThreshold)).onChange(async (value) => {
        const num = Number(value);
        this.plugin.settings.silenceThreshold = Number.isFinite(num) && num >= 0 ? num : 15e-4;
        await this.plugin.saveSettings();
        this.plugin.applyTuningSettings();
      })
    );
    new import_obsidian.Setting(containerEl).setName(t("settingBufferName")).setDesc(t("settingBufferDesc")).addText(
      (text) => text.setPlaceholder("12").setValue(String(this.plugin.settings.maxBufferedChunks)).onChange(async (value) => {
        const num = Number(value);
        this.plugin.settings.maxBufferedChunks = Number.isFinite(num) && num > 0 ? Math.round(num) : 12;
        await this.plugin.saveSettings();
        this.plugin.applyTuningSettings();
      })
    );
    new import_obsidian.Setting(containerEl).setName(t("settingInactivityName")).setDesc(t("settingInactivityDesc")).addText(
      (text) => text.setPlaceholder("0").setValue(String(this.plugin.settings.inactivityTimeoutMs / 1e3)).onChange(async (value) => {
        const numSeconds = Number(value);
        this.plugin.settings.inactivityTimeoutMs = Number.isFinite(numSeconds) && numSeconds >= 0 ? Math.round(numSeconds * 1e3) : 0;
        await this.plugin.saveSettings();
        this.plugin.applyTuningSettings();
      })
    );
    new import_obsidian.Setting(containerEl).setName(t("settingRepeatsName")).setDesc(t("settingRepeatsDesc")).addText(
      (text) => text.setPlaceholder("2").setValue(String(this.plugin.settings.maxConsecutiveWordRepeats)).onChange(async (value) => {
        const num = Number(value);
        this.plugin.settings.maxConsecutiveWordRepeats = Number.isFinite(num) && num >= 1 ? Math.round(num) : 2;
        await this.plugin.saveSettings();
        this.plugin.applyTuningSettings();
      })
    );
  }
};
